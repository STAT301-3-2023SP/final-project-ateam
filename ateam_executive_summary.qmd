---
title: "Executive Summary: What Makes a Car Exchangeable?"
subtitle: "Data Science 3 with R (STAT 301-3)"
author: "ateam: Allie Tong & Alani Cox-Caceres"


format:
  html:
    toc: true
    embed-resources: true
    code-fold: true
    echo: false
    link-external-newwindow: true
    
execute:
  warning: false
  message: false
  
from: markdown+emoji
---

## Github Repository
[https://github.com/STAT301-3-2023SP/final-project-ateam](https://github.com/STAT301-3-2023SP/final-project-ateam)

## Data Overview
Our [dataset](https://www.kaggle.com/datasets/lepchenkov/usedcarscatalog) consists of variables that describe cars in the used car market in Belarus on December 2, 2019. There are 38531 observations. Our target variable will be `is_exchangeable`, which is a factor variable that indicates `TRUE` if the used car can be exchanged with another car and `FALSE` if the used car cannot be exchanged with another car. There are 19 predictors, of which 1 is an integer, 5 are characters, 7 are numeric, and 6 are factors.

## Goal
In this exploration, we'd like to examine ways to predict if the used cars will be exchangeable based on the variables that affect that outcome the most. This is a categorical prediction problem. We will to explore what factors make a car exchangeable by testing the predictor importance of various predictor variables.

## Recipe Building
### Kitchen Sink
The kitchen sink recipe uses all of the variables in the dataset as predictors with `is_exchangeable` as the outcome variable. It will be used as a baseline to see if variable selection improves model performance. 

We first had to use `step_other()` for `model_name` to deal with the large number of levels the variable had. Then, we dummy encoded all nominal predictors. After, we removed the variables with zero variance and centered and scaled all variables. Lastly, we used `step_impute_knn()` to impute missingess and `step_corr()` to remove variables that have large correlations with other ones.

There is a full kitchen sink recipe and a shortened kitchen sink recipe. The full kitchen sink recipe is performed on the training set of 26971 variables, while the shortened kitchen sink recipe is performed on a portion of the training set of 4045 variables. For tuning, the full kitchen sink recipe was used with 10 folds and 5 repeats, while the shortened kitchen sink recipe was used with 5 folds and 3 repeats.

### Relationship Recipe
The relationship recipe uses the variables `odometer_value`, `year_produced`, `engine_capacity`, `price_usd`, `number_of_photos`, `engine_has_gas`, `has_warranty`, `state`, `drivetrain`, `location_region`, and `manufacturer_name`. These 11 variables showed possible relationships with the outcome variable `is_exchangeable`.

We first dummy encoded all nominal predictors. After, we removed the variables with zero variance and centered and scaled all variables. Lastly, we used `step_impute_knn()` to impute missingess and `step_corr()` to remove variables that have large correlations with other ones.

Similar to the kitchen sink model, there is a full relationship recipe and a shortened relationship recipe. The full relationship recipe is performed on the training set of 26971 variables, while the shortened relationship recipe is performed on a portion of the training set of 4045 variables. For tuning, the full relationship recipe was used with 10 folds and 5 repeats while the shortened relationship recipe was used with 5 folds and 3 repeats.

### Importance Recipe
The importance recipe includes variables that had nonzero importance using random forest variable selection. After tuning the random forest model with the kitchen sink model, we looked at each variable's importance. Displayed below is a table of the variables used and their importance in the random forest model.

## Models Chosen + Parameters
The models we will be fitting are:

- **Null Model** (to use as a baseline):
  - Doesn't have any main arguments

- **Random Forest Model:**
  - `min_n`
  - `mtry`

- **Boosted Tree Model:**
  - `min_n`
  - `mtry`
  - `learn_rate`

- **K Nearest Neighbors Model:**
  - `neighbors`

- **Elastic Net Model:**
  - `penalty`
  - `mixture`

- **Logistic Regression Model:**
  - `penalty`
  - `mixture`

- **SVM Poly Model:**
  - `cost`
  - `degree`
  - `scale_factor`

- **SVM Radial Model:**
  - `cost`
  - `rbf_sigma`

- **Multilayer Perceptron Model:**
  - `hidden_units`
  - `penalty`

- **MARS Model:**
  - `num_terms`
  - `prod_degree`

## Assessment Measures
We will use `roc_auc` as our performance metric for model performance. Once we have picked our final model and fitted it to our testing set, we will look at`roc_auc`, `accuracy`, and visualize a confusion matrix as well.

## Final Model Selection
Overall, the tree methods seemed to perform the best on this dataset with the random forest and boosted tree models producing the best results. Also, the kitchen sink recipe did better than the relationship recipe, implying that the benefits of more predictors used outweighed the benefits of only using the predictors that displayed relationships with the outcome variable. The importance recipe combined the two, by using more predictors than the relationship recipe and only using variables with predictive importance. However, in the end, the kitchen sink recipe did slightly better than the importance recipe with the random forest model. We think that this may be due to the importance recipe using fewer folds and repeats to shorten the runtime.

We expected the svm models to do better than they did. We believe that the reasoning behind the svm performing worse than expected is also due to the need to use fewer folds and repeats and to cut down the training set. When we originally ran the svm models, the models were running for over 48 hours. So, we decided to cut down the number of folds and repeats and the recipe to save runtime.

The final model we will use is the random forest with the kitchen sink recipe. We are not surprised, as random forest models typically perform well. However, we are surprised that the kitchen sink recipe performed better than the importance recipe. Again, it is likely due to the number folds and repeats used for each recipe and model.

### Comparing Final Model to Null Model
Our final model did better than the null model when comparing their area under the ROC curve, but not as well when comparing their accuracy. When comparing their accuracy, our final model performed similarly to the null model. Overall, our final model did significantly better than the null model, so we think it is worth the effort of building a predictive model. However, the runtime is very long, so we would need find ways to shorten the runtime. In shortening the runtime, we need to be careful to not worsen model performance, as using fewer folds and repeats or shortening the training set can cause model predictions to be less accurate.

## Conclusion
In conclusion, the final model performed well but did not fit our purposes to determine what variables make a car exchangeable. Our final model was a kitchen sink model that used all of the variables as predictors. Our relationship recipe that used graph variable selection methods had relatively poor model performance. The importance recipe performed slightly worse with the random forest model. This may have been due to the random forest with the importance recipe using fewer folds and repeats than the random forest model with the kitchen sink recipe.

Our next steps are to run the importance recipe on the rest of the models. We will then adjust tuning parameters based on the trends that we observed that led to higher roc_auc. Overall, we think that we need more variables in the dataset to choose from, as there are only 20 right now. In having more variables, we think our variable selection methods will be more successful, as the benefit of the KS model using more predictors is not as important. We will then be able to look at what factors make a car exchangeable.
