---
title: "Final Report"
subtitle: "Data Science 3 with R (STAT 301-3)"
author: "ateam: Allie Tong, Alani Cox-Caceres"


format:
  html:
    toc: true
    embed-resources: true
    code-fold: show
    link-external-newwindow: true
    echo: false
    
execute:
  warning: false

from: markdown+emoji 
---

## Github Repository
[https://github.com/STAT301-3-2023SP/final-project-ateam](https://github.com/STAT301-3-2023SP/final-project-ateam)

## Data Overview
Our [dataset](https://www.kaggle.com/datasets/lepchenkov/usedcarscatalog) consists of variables that describe cars in the used car market in Belarus on December 2, 2019. There are 38531 observations. Our target variable will be `is_exchangeable`, which is a factor variable that indicates `TRUE` if the used car can be exchanged with another car and `FALSE` if the used car cannot be exchanged with another car. There are 19 predictors, of which 1 is an integer, 5 are characters, 7 are numeric, and 6 are factors.

## Goal
In this exploration, we'd like to examine ways to predict if the used cars will be exchangeable based on the variables that affect that outcome the most. This is a categorical prediction problem. We want to explore what factors make a car exchangeable and will test the predictor importance of various predictors to do so.

## Data Splitting and Resampling Plan
As we mentioned in our initial exploration, our data was relatively unbalanced. To remedy this, we stratified our target variable, `is_exchangeable`, and used a 70/30 split for our training and testing data. For resampling, we used a cross-validation method. For the models that have shorter runtime, we will use 10 folds with 5 repeats. For the models that have longer runtime, we will use 5 folds and 3 repeats.

## Feature Engineering
- We first ensured that there weren't any significant missing issues that could not be imputed and found no significant missingness.

- We looked at the relationships between each of the predictors and the outcome variable through boxplots and bar graphs. From looking at the relationships, we narrowed one of our recipes to 11 predictors. We plan to test if this improves model performance.

- All of the feature engineering was done on a portion of the training set.

## Recipe Building
### Kitchen Sink

### Rec_Rel


## Models Chosen + Parameters


## Assessment Measures
The assessment measures we chose to test model performance are accuracy, roc_auc, and a confusion matrix. 

* Accuracy: accuracy measures the proportion of correctly classified predictions over the total number of predictions 

* ROC_AUC: a roc_auc curve measures the probability that any randomly identified positive prediction in ranked higher by the model than a randomly identified negative prediction. It produces an overall evaluation of the model's performance

* Confusion Matrix: a confusion matrix is used to create a table that summarizes the predictions made in a classification model. It identifies the number of true positives, true negatives, false positives, and false negatives.


## Model Performance


## Issues


## Final Model Analysis


