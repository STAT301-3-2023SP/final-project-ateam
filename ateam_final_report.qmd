---
title: "Final Report"
subtitle: "Data Science 3 with R (STAT 301-3)"
author: "ateam: Allie Tong & Alani Cox-Caceres"


format:
  html:
    toc: true
    embed-resources: true
    code-fold: show
    link-external-newwindow: true
    echo: false
    
execute:
  warning: false

from: markdown+emoji 
---

## Github Repository
[https://github.com/STAT301-3-2023SP/final-project-ateam](https://github.com/STAT301-3-2023SP/final-project-ateam)

## Data Overview
Our [dataset](https://www.kaggle.com/datasets/lepchenkov/usedcarscatalog) consists of variables that describe cars in the used car market in Belarus on December 2, 2019. There are 38531 observations. Our target variable will be `is_exchangeable`, which is a factor variable that indicates `TRUE` if the used car can be exchanged with another car and `FALSE` if the used car cannot be exchanged with another car. There are 19 predictors, of which 1 is an integer, 5 are characters, 7 are numeric, and 6 are factors.
```{r}
library(tidyverse)
library(tidymodels)

tidymodels_prefer()

# set seed ----
set.seed(1234)

# load in data ---- 
cars <- read_rds(file = "data/processed/cars_clean.rds")

# split data ----
cars_split <- initial_split(cars, 
                            strata = is_exchangeable, 
                            prop = 0.7)

cars_train <- training(cars_split)
cars_test <- testing(cars_split)

# test for balance
ggplot(cars_train, mapping = aes(x = is_exchangeable, fill = is_exchangeable)) +
  geom_bar() +
  theme_minimal()
```


## Goal
In this exploration, we'd like to examine ways to predict if the used cars will be exchangeable based on the variables that affect that outcome the most. This is a categorical prediction problem. We will to explore what factors make a car exchangeable by testing the predictor importance of various predictor variables.

## Data Splitting and Resampling Plan

```{r}
ggplot(cars_train, mapping = aes(x = is_exchangeable, fill = is_exchangeable)) +
  geom_bar() +
  theme_minimal()
```
As seen above, our data was relatively unbalanced. To remedy this, we stratified our target variable, `is_exchangeable`, and used a 70/30 split for our training and testing data. For resampling, we used a cross-validation method. For the models that have shorter runtime, we will use 10 folds with 5 repeats. For the models that have longer runtime, we will use 5 folds and 3 repeats.

## Feature Engineering
```{r}
library(naniar)
miss_var_summary(cars)
```

As seen above, there are no significant missing issues that could not be imputed and we found no significant missingness.

```{r}
load(file = "results/num_rel")
gridExtra::grid.arrange(grobs = box_plots_num)
```

```{r}
load(file = "results/nom_rel")

nom_rel_1

nom_rel_2

nom_rel_3
```
Using the graphs above, we looked at the relationships between each of the predictors and the outcome variable. From looking at the relationships, we narrowed our relationship recipe to the predictors that displayed a relationship with the outcome variable. We plan to test if this improves model performance.

All of the feature engineering was done on a portion of the training set.

## Recipe Building
### Kitchen Sink
The kitchen sink recipe uses all of the variables in the dataset as predictors with `is_exchangeable` as the outcome variable. It will be used as a baseline to see if variable selection improves model performance. 

We first had to use `step_other()` for `model_name` to deal with the large number of levels the variable had. Then, we dummy encoded all nominal predictors. After, we removed the variables with zero variance and centered and scaled all variables. Lastly, we used `step_impute_knn()` to impute missingess and `step_corr()` to remove variables that have large correlations with other ones.

There is a full kitchen sink recipe and a shortened kitchen sink recipe. The full kitchen sink recipe is performed on the training set of 26971 variables, while the shortened kitchen sink recipe is performed on a portion of the training set of 4045 variables. For tuning, the full kitchen sink recipe was used with 10 folds and 5 repeats, while the shortened kitchen sink recipe was used with 5 folds and 3 repeats.

### Relationship Recipe
The relationship recipe uses the variables `odometer_value`, `year_produced`, `engine_capacity`, `price_usd`, `number_of_photos`, `engine_has_gas`, `has_warranty`, `state`, `drivetrain`, `location_region`, and `manufacturer_name`. These 11 variables showed possible relationships with the outcome variable `is_exchangeable`.

We first dummy encoded all nominal predictors. After, we removed the variables with zero variance and centered and scaled all variables. Lastly, we used `step_impute_knn()` to impute missingess and `step_corr()` to remove variables that have large correlations with other ones.

Similar to the kitchen sink model, there is a full relationship recipe and a shortened relationship recipe. The full relationship recipe is performed on the training set of 26971 variables, while the shortened relationship recipe is performed on a portion of the training set of 4045 variables. For tuning, the full relationship recipe was used with 10 folds and 5 repeats while the shortened relationship recipe was used with 5 folds and 3 repeats.

### Importance Recipe
The importance recipe includes variables that had nonzero importance using random forest variable selection. After tuning the random forest model with the kitchen sink model, we looked at each variable's importance. Displayed below is a table of the variables used and their importance in the random forest model.

```{r}
library(tidyverse)
library(kableExtra)
load(file = "results/rf_vars.rda")
rf_vars %>% 
  kbl() %>% 
  kable_styling()
```

As seen above, all of the variables used in the relationship recipe have importance. The importance recipe includes 6 more predictors than the relationship recipe.

We first dummy encoded all nominal predictors. After, we removed the variables with zero variance and centered and scaled all variables. Lastly, we used `step_impute_knn()` to impute missingess and `step_corr()` to remove variables that have large correlations with other ones.

We will not use the importance recipe for all of the models, but we will run the winning model with it to see if the importance recipe does better.

## Models Chosen + Parameters
The models we will be fitting are:

- **Null Model** (to use as a baseline): A simple, non-informative model
  - Doesn't have any main arguments

- **Random Forest Model:** A model that creates a large number of decision trees, each independent of the others. It involves stratifying the predictor space into a number of simple regions. The predictions typically use the mean or mode response value in the region it belongs.
  - Tuning parameters:
    - `min_n`: The number of predictors that will be randomly sampled at each split when creating the tree models
    - `mtry`: The number of predictors that will be randomly sampled at each split when creating the tree models
      - Set an upper bound of 17 (max number of predictors a recipe would have)

- **Boosted Tree Model:** A model that creates a series of decision trees forming an ensemble. Each tree depends on the results of previous trees. All trees in the ensemble are combined to produce a final prediction.
  - Tuning parameters: 
    -`min_n`: The minimum number of data points in a node that is required for the node to be split further
    - `mtry`: The number (or proportion) of predictors that will be randomly sampled at each split when creating the tree models
      - Set an upper bound of 17 (max number of predictors a recipe would have)
    - `learn_rate`: The rate at which the boosting algorithm adapts from iteration-to-iteration

- **K Nearest Neighbors Model:** A model that uses the *K* most similar data points from the training set to predict new samples
  - Tuning parameters: 
    - `neighbors`: The number of neighbors to consider

- **Elastic Net Model:** A model that uses linear predictors to predict multiclass data using the multinomial distribution
  - Tuning parameters: 
    - `penalty`: A non-negative number representing the total amount of regularization
    - `mixture`: A number between zero and one (inclusive) giving the proportion of L1 regularization
      - Elastic net model interpolates lasso and ridge with 0 < `mixture` < 1

- **Logistic Regression Model:** This model uses a linear combination of the predictors to calculate or predict the probability of an event occurring.
  - Tuning parameters: 
    - `penalty`: A non-negative number representing the total amount of regularization
    - `mixture`: A number between zero and one (inclusive) giving the proportion of L1 regularization
      - `mixture` = 1: pure lasso model
      - `mixture` = 0: ridge regression model

- **SVM Poly Model:** The model tries to maximize the width of the margin between classes using a nonlinear class boundary.
  - Tuning parameters:
    - `cost`: A positive number for the cost of predicting a sample within or on the wrong side of the margin
    - `degree`: A positive number for polynomial degree
    - `scale_factor`: A positive number for the polynomial scaling factor

- **SVM Radial Model:** The model tries to maximize the width of the margin between classes using a polynomial class boundary.
  - Tuning parameters:
    - `cost`: A positive number for the cost of predicting a sample within or on the wrong side of the margin
    - `rbf_sigma`: A positive number for radial basis function

- **Multilayer Perceptron Model:** A neural network where the mapping between inputs and output is non-linear
  - Tuning parameters:
    - `hidden_units`: An integer for the number of units in the hidden model
    - `penalty`: A non-negative numeric value for the amount of weight decay

- **MARS Model:** A model that uses an algorithm that involves finding a set of simple linear functions to predict complex non-linear problems
  - Tuning parameters:
    - `num_terms`: The number of features that will be retained in the final model, including the intercept
    - `prod_degree`: The highest possible interaction degree

## Assessment Measures
We will use `roc_auc` as our performance metric for model performance. Once we have picked our final model and fitted it to our testing set, we will look at`roc_auc`, `accuracy`, and visualize a confusion matrix as well.

* Accuracy: accuracy measures the proportion of correctly classified predictions over the total number of predictions 

* ROC_AUC: a roc_auc curve measures the probability that any randomly identified positive prediction is ranked higher by the model than a randomly identified negative prediction. It produces an overall evaluation of the model's performance

* Confusion Matrix: a confusion matrix is used to create a table that summarizes the predictions made in a classification model. It identifies the number of true positives, true negatives, false positives, and false negatives.


## Model Performance
Displayed below is a graph of model results.

As said before, the long recipes have 10 folds and 5 repeats, and are performed on the training set of 26971. The short recipes have 5 folds and 3 repeats, and are performed on a portion of the training set of 4045 variables. However, the short recipe for the random forest importance recipe has 5 folds and 3 repeats and is performed on the entire training set.

We tuned a random forest model with the importance recipe, as the random forest model with the kitchen sink recipe had the lowest `roc_auc`. The random forest with the importance recipe performed slightly worse than the random forest with the kitchen sink recipe.
```{r}
load(file = "results/model_results")
result_table %>% 
  kbl() %>% 
  kable_styling()
```

Next, we will visualize the best performing models. We will be doing this with all of the models that had an `roc_auc` over `0.65`.

**Kitchen Sink Recipe (long)**
```{r}
load(file = "results/result_graphs")
result_ks_graph_long
```

**Kitchen Sink Recipe (short)**
```{r}
result_ks_graph_short
```

**Relationship Recipe**
```{r}
result_rel_graph
```

**Importance Recipe**
```{r}
result_imp_graph
```

All of the models have small confidence intervals, except the neural network with the shortened kitchen sink recipe and the random forest with the importance recipe.

## Tuning Parameters
We tuned at fit 9 models in our exploration. Aside from the null model, we added additional parameters to our other models to further improve their performance. Each of these graphs shows the roc_auc curve for each model in both the kitchen sink nd relationship recipes. 
- As mentioned in the Assessment Measures section, the roc_auc curve provides a visual representation of the relationship between the true positive rate (sensitivity) and the false positive rate (1-specificity).

Here are the roc_auc curves for each model:

### Kitchen Sink Recipe

- **Random Forest Model:**
```{r}
load(file = "results/tuning_rf_ks.rda")

autoplot(rf_tune_ks, metric = "roc_auc")
```
For this model, it seems that it has the best roc_auc score when it has around 5 randomly selected predictors. When this occurs, the roc_auc score for this model is relatively high, but otherwise, it doesn't perform very well.

- **Boosted Tree Model:**
```{r}
load(file = "results/tuning_boost_ks.rda")

autoplot(boost_tune_ks, metric = "roc_auc")
```
This model looks very similar to the random forest model, and produces the same advantages as that model. It is a bit shorter than the random forest, and the curve is not as concave, so its performance isn't as great relative to the random forest, but this model still performs relatively well.

- **K Nearest Neighbors Model:**
```{r}
load(file = "results/tuning_knn_ks.rda")

autoplot(knn_tune_ks, metric = "roc_auc")
```
The roc_auc curve for this model has a very high end point towards the top-right corner, which means it is less likely to falsely identify a negative case as positive, but the peak of the curve isn't very close to the top-left corner, which indicates that it has a relatively low proportion of actual positive cases that were correctly classified as positive.

This model also has a high number of neighbors, which means it can take into account a larger number of data points from different classes, but this also means it may be more. robust and could lose information for some of the smaller details. 
    
- **Elastic Net Model:**
```{r}
load(file = "results/tuning_en_ks.rda")

autoplot(en_tune_ks, metric = "roc_auc")
```
This model faces the opposite way of the previous model, which means it is highly proficient in correctly identifying positive cases, but the peak of the curve approaches the upper-right corner, which means this model is poor at avoiding false positive errors.

However, this graph also tests the amount of regularization. Regularization is a method that is used to prevent overfitting. This model has high regularization, which means means the model is more restricted, which may help to prevent overfitting.

- **Logistic Regression Model:**
```{r}
load(file = "results/tuning_log_reg_ks.rda")

autoplot(log_reg_tune_ks, metric = "roc_auc")
```
This graph looks very similar to the previous one. It indicates that this model has high proficiency in correctly identifying positive cases, poor proficiency in avoiding false positive errors, but is relatively good at avoiding overfitting. 

- **MARS Model:**
```{r}
load(file = "results/tuning_mars_ks.rda")

autoplot(mars_tune_ks, metric = "roc_auc")
```
This model has a high proficiency in correctly identifying positive cases, but performs relatively poorly when attempting to avoid false positive cases.

- **Multilayer Perceptron Model:**
```{r}
load(file = "results/tuning_nn_ks.rda")

autoplot(nn_tune_ks, metric = "roc_auc")
```
This model contains a multitude of results, but the general shape of the curves indicates that this model has a high proportion of correctly identified positive cases, but is very poor at avoiding false positive errors. 

It also has a high number of hidden units, which means it has a more complex model structure with more patterns and representations. 

Even so, the results are varied, so this model is a poor representation of a good roc_auc score. 

- **SVM Poly Model:**
```{r}
load(file = "results/tuning_svm_poly_ks.rda")

autoplot(svm_poly_tune_ks, metric = "roc_auc")
```
The SVM Poly Model contains different degrees of interaction. Each of these degrees refers to the polynomial degree that is used in the kernel function. The kernel function determines how the data is transformed and separated in a higher-dimensional feature space. Degree 1 is a linear kernel, degree 2 is a quadratic kernel, and degree 3 is a higher-degree polynomial kernel. 

These graphs also test the cost parameter. The cost parameter determines the penalty for incorrectly classifying training examples, and influences the width and number of support vectors in an SVM Poly Model.

In each of these graphs, the results vary heavily among each of the scale factors. This means that although this model can perform well, it's overall roc_auc score is relatively low.

- **SVM Radial Model:**
```{r}
load(file = "results/tuning_svm_rad_ks.rda")

autoplot(svm_rad_tune_ks, metric = "roc_auc")
```
In an SVM Radial model, the radial basis function parameter sigma determines the width of the kernel that is used to transform the data into a higher-dimensional feature space. 

This model performs similarly to the previous one in that the results are quite varied among each radial basis function sigma. This means that overall, the model has relatively low performance when tested using the roc_auc metric, and has varied widths with each kernel. 

### Relationship Recipe

- **Random Forest Model:**
```{r}
load(file = "results/tuning_rf_rel.rda")

autoplot(rf_tune_rel, metric = "roc_auc")
```
For this recipe, it seems that the random forest model has the best roc_auc score when it has around 5 randomly selected predictors. When this occurs, the roc_auc score for this model is relatively high, but otherwise, it doesn't perform very well.

- **Boosted Tree Model:**
```{r}
load(file = "results/tuning_boost_rel.rda")

autoplot(boost_tune_rel, metric = "roc_auc")
```
Similar to the model above, the boosted tree model has the best roc_auc score when the number of randomly selected predictors is 5. 

- **K Nearest Neighbors Model:**
```{r}
load(file = "results/tuning_knn_rel.rda")

autoplot(knn_tune_rel, metric = "roc_auc")
```
This model has a high roc_auc score, which means it maintains a good balance of correctly identified positive cases, and identifying false positive cases. It also has the best performance with a higher number of neighbors. 
        
- **Elastic Net Model:**
```{r}
load(file = "results/tuning_en_rel.rda")

autoplot(en_tune_rel, metric = "roc_auc")
```
The elastic net model has the best roc_auc score when the amount of regularization is between 0 and 1e-02. In this range, the roc_auc score is highest, and the amount of regularization means the model is also is relatively more proficient at preventing overfitting.
        
- **Logistic Regression Model:**
```{r}
load(file = "results/tuning_log_reg_rel.rda")

autoplot(log_reg_tune_rel, metric = "roc_auc")
```
This model performs similarly to the previous on with the highest roc_auc score occurring when the amount of regularization remains between 0 and 1e-02.
        
- **MARS Model:**
```{r}
load(file = "results/tuning_mars_rel.rda")

autoplot(mars_tune_rel, metric = "roc_auc")
```
For both degrees of interaction, this model has the highest roc_auc score with 5 model terms. The model terms refer to the individual functions that make up the MARS model, so in this case, the more components the MARS model has, the better its roc_auc performance will be.

- **Multilayer Perceptron Model:**
```{r}
load(file = "results/tuning_nn_rel.rda")

autoplot(nn_tune_ks, metric = "roc_auc")
```
Depending on which layer of the model is being observed, this model has varying degrees of roc_auc performance as well as what number of hidden units determines the best roc_auc performance. Because these results are so varied, it shows that the overall performance of this model is relatively poor.

- **SVM Poly Model:**
```{r}
load(file = "results/tuning_svm_poly_rel.rda")

autoplot(svm_poly_tune_ks, metric = "roc_auc")
```
Similar to the model above, within each kernel, this model has extremely varied results. The different scale factors do not have a cohesive cost paramter that has the best roc_auc score, so overall, this model has a relatively low roc_auc score. 

- **SVM Radial Model:**
```{r}
load(file = "results/tuning_svm_rad_rel.rda")

autoplot(svm_rad_tune_ks, metric = "roc_auc")
```
This model is also varied in its roc_auc score for each radial basis function sigma, which means it has a relatively poor overall roc_auc score. 

### Importance Recipe

- **Random Forest Model:**
```{r}
load(file = "results/tuning_rf_imp.rda")

autoplot(rf_tune_imp, metric = "roc_auc")
```
After examining all of the different models, it seems that the random forest model had the best overall performance with the roc_auc score. This means it has the best balance between correctly identifying positive cases and avoiding false positive cases. When plotted using the importance recipe, the random forest model sstill performs very well in both of these areas, further affirming that this is model has the best overall performance.

## Final Model Selection
Overall, the tree methods seemed to perform the best on this dataset with the random forest and boosted tree models producing the best results. Also, the kitchen sink recipe did better than the relationship recipe, implying that the benefits of more predictors used outweighed the benefits of only using the predictors that displayed relationships with the outcome variable. The importance recipe combined the two, by using more predictors than the relationship recipe and only using variables with predictive importance. However, in the end, the kitchen sink recipe did slightly better than the importance recipe with the random forest model. We think that this may be due to the importance recipe using fewer folds and repeats to shorten the runtime.

We expected the svm models to do better than they did. We believe that the reasoning behind the svm performing worse than expected is also due to the need to use fewer folds and repeats and to cut down the training set. When we originally ran the svm models, the models were running for over 48 hours. So, we decided to cut down the number of folds and repeats and the recipe to save runtime.

The final model we will use is the random forest with the kitchen sink recipe. We are not surprised, as random forest models typically perform well. However, we are surprised that the kitchen sink recipe performed better than the importance recipe. Again, it is likely due to the number folds and repeats used for each recipe and model.

## Final Model Analysis
Now that we have chosen our best model, we will complete our final model workflow and fit it to the testing data.

The table below shows the observations of `is_exchangeable` in the testing data and compares it to what the final model predicted its `is_exchangeable` value would be. 

- Class probabilities are an estimation of the probability that an observation belongs to each class for `is_exchangeable` (`TRUE` or `FALSE`) in a set of classes (`TRUE` and `FALSE`). 

So, `.pred_TRUE = 0.51` and `.pred_FALSE = 0.49` would imply that our model had trouble classifying the car, whereas `.pred_TRUE = 0.95` and `.pred_FALSE = 0.05` would imply that my model confidently classified a car as being exchangeable.

```{r}
load(file = "results/final_result")
final_result %>% 
  rename(prediction = .pred_class) %>% 
  kbl() %>% 
  kable_styling()
```

### ROC Curve
The ROC curve shows the trade-off between sensitivity (or true positive rate) and specificity (1 – true positive rate). In this case, the true positive observation would be `is_exchangeable = FALSE`. The true positive rate is the proportion of observations that were correctly predicted to be positive out of all positive observations. 

The closer to the top left corner the curve is, the better the model performance. As a baseline, a null model is expected to give points lying along the dotted diagonal line. So, it seems that our model did significantly better than the null model.

```{r}
final_result %>% 
  roc_curve(is_exchangeable, .pred_FALSE) %>% 
  autoplot()
```

### Area Under ROC Curve
The area under the ROC curve of our final model is the probability that the model correctly classifies a car that is exchangeable from a car that isn't. It provides an aggregate measure of performance across all possible classification thresholds. A value of `0.715` means that our random forest model with the kitchen sink recipe did pretty well in correctly classifying the cars in the testing set, though there is still some room for error. It performed similarly on the testing set as it did in the training set.

```{r}
set.seed(1234)

final_result %>% 
  roc_auc(is_exchangeable, .pred_FALSE) %>% 
  kbl() %>% 
  kable_styling()
```

### Accuracy
Our final model was able to accurately predict 70% of the observations in my testing set. This means that my model was able to correctly predict whether a car was exchangeable or not for 8111 out of the 11560 cars in the testing set.

```{r}
set.seed(1234)

accuracy(final_result, is_exchangeable, .pred_class) %>% 
  kbl() %>% 
  kable_styling()
```

### Confusion Matrix
The confusion matrix shows how accurate our final model predictions are. So, in the `(Prediction FALSE, Truth FALSE)` category, these are the amount of observations that we correctly predicted were `FALSE`. In the `(Prediction FALSE, Truth TRUE)` category, these are the amount of observations that we incorrectly predicted were `FALSE`. In the `(Prediction TRUE, Truth FALSE)`, these are the amount of observations that Iwe incorrectly predicted were `TRUE`. In the `(Prediction TRUE, Truth TRUE)`, these are the amount of observations that we correctly predicted were `TRUE`. Essentially, it is a visual representation of the performance metric accuracy.

We see that our final model did better at predicting the cars that were not exchangeable than predicting cars that were. It correctly classified 90.61% of the cars that were not exchangeable, and only 67.35% of the cars that were exchangeable. This issue can be partially attributed to the imbalance of `is_exchangeable`, as there were around 2x more `FALSE` observations than `TRUE`.

```{r}
conf_mat(final_result, is_exchangeable, .pred_class)
```

### Comparing Final Model to Null Model
```{r}
load(file = "results/tuned_null.rda")
best_null_auc <- show_best(null_fit, metric = "roc_auc")[1,]
best_null_accuracy <- show_best(null_fit, metric = "accuracy")[1,]

null_vs_rf <- tibble(model = c("Null", "Null", "RF", "RF"),
       metric = c("ROC", "Accuracy", "ROC", "Accuracy"),
       mean = c(best_null_auc$mean, best_null_accuracy$mean, 
                "0.7151476", "0.7017301"))

null_vs_rf %>% 
  kbl() %>% 
  kable_styling()
```
Our final model did better than the null model when comparing their area under the ROC curve, but not as well when comparing their accuracy. When comparing their accuracy, our final model performed similarly to the null model. Overall, accuracy is based on one specific cutpoint, while ROC tries all of the cutpoint and plots and the sensitivity and specificity. So when we compare the overall accuracy, we are comparing the accuracy based on a designated cutpoint. The overall accuracy varies from different cutpoints. Therefore, our final model's accuracy being worse than its area under the ROC curve may be due to choosing a cutpoint that yields a lower accuracy.

In conclusion, the final model performed well but did not fit our purposes to determine what variables make a car exchangeable because our final model ended up being a kitchen sink model that used all of the variables as predictors. The recipes that used variable selection methods had relatively poor model performance when applied. To remedy this, ee think that we need more variables in the dataset to choose from, since right now there are only 20.

